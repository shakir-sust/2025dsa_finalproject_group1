---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script contains the data wrangling and feature engineering steps for the final project.

# Start of data wrangling

## Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```

## Reading "training" data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

submission_test <- read_csv("../data/testing/testing_submission.csv")
soil_test <- read_csv("../data/testing/testing_soil.csv")
meta_test <- read_csv("../data/testing/testing_meta.csv")


```


## Data wrangling on "training" data

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}

trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
  ) %>%
  select(-yield_mg_ha, -grain_moisture) %>%
  ungroup()


trait_clean

```

The following code chunks will conduct data wrangling on "trait_clean"

```{r}

submission_test_clean <- submission_test %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  select(-yield_mg_ha) %>%
  ungroup()

submission_test_clean

```


The following code chunks will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_clean
```

The following code chunks will conduct data wrangling on  "soil_test_clean"

```{r}

soil_test_clean <- soil_test %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_test_clean

```

The following code chunks will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_clean <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_clean

```

The following code chunks will conduct data wrangling on "meta_test_clean"

```{r}

meta_test_clean <- meta_test %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_test_clean

```


## EDA for cleaned data

The following code chunk will conduct EDA for cleaned data.

```{r}
summary(trait_clean)

summary(soil_clean)

summary(meta_clean)

summary(submission_test_clean)

summary(soil_test_clean)

summary(meta_test_clean)

```


## Merging all 3 cleaned data frames for training data

The following code chunk will merge all 3 cleaned data frames for training data.

```{r}

merged_train <- trait_clean %>%
  left_join(soil_clean, by = c("year","site")) %>%
  left_join(meta_clean, by = c("year","site"))


merged_test <- submission_test_clean %>%
  left_join(soil_test_clean, by = c("year","site")) %>%
  left_join(meta_test_clean, by = c("year","site"))


merged_all <- bind_rows(merged_train, merged_test)

merged_all

```


## EDA for cleaned merged data

The following code chunk will conduct EDA on the merged data.

```{r}

summary(merged_all)

```

 Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


The following code chunk will create a map of the USA and plot the sites in the map based on their latitude and longitude.


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii)
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_all,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```


The following code chunk will keep the observations for site-years having latitude and longitude withing the Daymet range of co-ordinates.

Declaration of AI use: the following code chunk was inspired and subsequently adapted on the basis of code initially generated by ChatGPT.


```{r}

# Defining Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filtering merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_all %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Reporting how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_all) - nrow(merged_daymet), ")")


```


The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```


The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


weather_daymet_all



```


The following code chunk will unnest the "weather_daymet_all" data.

```{r}

daymet_all_unnest <- weather_daymet_all %>%
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names() 

daymet_all_unnest




```

The following code chunk will export and save the "fieldweatherdata.csv" data.

```{r}

write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )
```

The following code chunk will import the "fieldweatherdata.csv" data and assign it to "fieldweather" object.

```{r}

fieldweather <- read_csv("../data/fieldweatherdata.csv")

fieldweather

```
















