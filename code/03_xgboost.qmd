---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script contains ML Workflow with XGBoost

# Loading packages

The following code chunk will load necessary packages.

```{r}

# 1. Setup 

#install.packages("tidymodels")   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
#install.packages("finetune")     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
#install.packages("vip")          # For plotting variable importance from fitted models
#install.packages("xgboost")      # XGBoost implementation in R
#install.packages("ranger")       # Fast implementation of Random Forests
#install.packages("tidyverse")    # Data wrangling and visualization
#install.packages("doParallel")   # For parallel computing (useful during resampling/tuning)
#install.packages("caret")  
#install.packages("xgboost") #new pacakage
#install.packages("caret")
#install.packages("yardstick")

library(tidymodels)   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
library(finetune)     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
library(vip)          # For plotting variable importance from fitted models
library(xgboost)      # XGBoost implementation in R
library(ranger)       # Fast implementation of Random Forests
library(tidyverse)    # Data wrangling and visualization
library(doParallel)   # For parallel computing (useful during resampling/tuning)
library(caret)       # Other great library for Machine Learning 
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(stringr)
library(tidymodels)
library(xgboost)
library(vip)
library(finetune)
library(doParallel)
library(parallel)
library(beepr)
library(yardstick)  

```

# Loading the data set

The following code chunk will load the "weather_monthsum.csv" data set.

```{r weather}

# Data import 

weather_xgb <- read_csv("../data/weather_monthsum.csv") %>%
  rename(yield = adjusted_yield)

weather_xgb

```

# EDA plots on soil predictors

The following code chunk will create EDA plots on soil predictors.

```{r ridge_soil_selected_fixed, message=FALSE, warning=FALSE}

library(ggridges)   
library(viridis)    
library(purrr)       
library(ggplot2)  

var_labels <- c(
  soilpH = "soil pH",
  om_pct = "soil organic matter"
)

walk2(
  names(var_labels),    
  var_labels,           
  function(var, label) {
    # computing density to normalize height
    dens <- density(weather_xgb[[var]], na.rm = TRUE)
    scale_val <- 1 / max(dens$y, na.rm = TRUE)
    
    # building the ggplot object
    p <- ggplot(weather_xgb, aes(x = .data[[var]], y = label, fill = stat(x))) +
      geom_density_ridges_gradient(
        scale = scale_val,
        rel_min_height = 0.01
      ) +
      scale_fill_viridis_c(option = "C") +
      labs(
        title = paste("Distribution of", label),
        x = "Mean soil pH between 2014 - 2024",
        y = NULL
      ) +
      theme_ridges() +
      theme(
        legend.position = "none",
        axis.text.y = element_blank()
      )
    print(p)
  }
)

```

The following code chunk will export "weather_xgb.csv".

```{r}

write_csv(weather_xgb,
          "../data/weather_xgb.csv")

```

# Splitting off 2024 for final prediction

The following code chunk will split off 2024 for final prediction 

```{r}

# Split off 2024 for final prediction 

weather_to_predict <- weather_xgb %>% filter(is.na(yield)) # holding out 2024 rows (no yield)
weather_to_predict

weather_complete <- weather_xgb %>% filter(!is.na(yield)) # keeping 2014–2023 for training
weather_complete 

```

# Train/test split

The following code chunk will conduct train/test split for the data of the years 2014–2023.

```{r}

# Train/test split (2014–2023) 

set.seed(931735) # reproducible split

xgb_split <- initial_split(weather_complete,  # creating split object
                           prop = 0.7, # 70% train
                           strata = yield) # stratifying on yield
xgb_split

weather_train_xgb <- training(xgb_split) # extracting training set
weather_train_xgb

weather_test_xgb  <- testing(xgb_split) # extracting test set
weather_test_xgb 

```

# Comparing the distribution of "yield" in training vs. test sets

The following code chunk will compare the distribution of "yield" in training vs. test sets.

```{r distribution_with_legend, message=FALSE, warning=FALSE}

library(ggplot2)
library(dplyr)

# 1. Combine train and test with a new 'dataset' column
yield_df <- bind_rows(
  weather_train_xgb %>% mutate(dataset = "Training"),
  weather_test_xgb  %>% mutate(dataset = "Test")
)

# 2. Plot densities with a legend
ggplot(yield_df, aes(x = yield, color = dataset)) +
  geom_density(size = 1) +
  scale_color_manual(
    values = c(Training = "red", Test = "blue"),
    name = "Data split"
  ) +
  labs(
    title = "Distribution of Yield",
    x = "Yield (Mg/ha)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12),
    legend.position = "top"
  )
```

The following code chunk will export "yield_df.csv"

```{r}

write_csv(yield_df,
          "../data/yield_df.csv")

```

# Pre-processing recipe

The following code chunk will conduct pre-processing recipe.

```{r}

# Pre-processing recipe 

weather_recipe_xgb <-
  recipe(yield ~ ., data = weather_train_xgb) %>% # targeting ~ all predictors
  step_rm(year, site, hybrid, # dropping IDs
          matches("Jan|Feb|Mar|Nov|Dec")) %>% # dropping off‐season months
  step_impute_median(all_numeric_predictors()) # median imputation for NAs

weather_recipe_xgb

```

# Model specification

The following code chunk will conduct model specification.

```{r}

# Model specification 

xgb_spec <-
  boost_tree(
    trees = tune(), # tuning number of boosting rounds
    tree_depth = tune() # tuning max tree depth
  ) %>%
  set_engine("xgboost") %>%  # using XGBoost backend
  set_mode("regression") # regression mode

xgb_spec

```

# Bundle recipe & model into workflow

The following code chunk will bundle recipe & model into workflow.

```{r}

# Workflow 

xgb_wf <- workflow() %>%
  add_recipe(weather_recipe_xgb) %>% # adding preprocessing
  add_model(xgb_spec)  # adding XGBoost spec

xgb_wf

```

# Resampling strategy

The following code chunk will setup Resampling plan.  

```{r}

# Resampling setup 

set.seed(34549)

resamples_xgb <- vfold_cv(
  weather_train_xgb, 
  v = 5 # 5 folds
  )

resamples_xgb

```

# Hyperparameter grid search with Latin Hypercube Sampling

The following code chunk will conduct hyperparameter grid search with Latin Hypercube Sampling.

```{r}

# Hyperparameter grid (Latin hypercube) 

xgb_grid <- grid_latin_hypercube(
  trees(), # tuning number of trees
  tree_depth(), # tuning tree depth
  size = 10
)

xgb_grid

```

# Registering a parallel cluster

The following code chunk will create and register a parallel cluster.

```{r}

# Parallel backend 

cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)

```

# Racing tune via ANOVA

The following code chunk will conduct racing tune via ANOVA

```{r}

# Racing tune via ANOVA 

set.seed(76544) # reproducible tuning

xgb_race_res <- tune_race_anova(
  object = xgb_spec, # model spec to tune
  preprocessor = weather_recipe_xgb, # recipe to apply
  resamples = resamples_xgb, # cross-validation object
  grid = xgb_grid, # hyperparameter grid
  control = control_race(save_pred = TRUE) # saving predictions
)

beepr::beep()

xgb_race_res
xgb_race_res$.metrics[[2]]

```

# Selecting best hyperparameters

The following code chunk will select best hyperparameters.

```{r}

# Select best parameters 

best_params_xgb <- xgb_race_res %>%
  select_best(metric = "rmse") %>% # choosing by RMSE
   mutate(source = "best_rmse")

best_params_xgb

```

# Finalizing workflow & fitting full training set

The following code chunk will finalize workflow & fit full training set.

```{r}

# Finalize workflow & fit on full training set 

final_xgb_wf  <- finalize_workflow(xgb_wf, best_params_xgb) # plugging in best params
final_xgb_wf

final_xgb_fit <- fit(final_xgb_wf, data = weather_train_xgb) # training on all 2014–2023 data
final_xgb_fit

```

# Stopping parallel processing

The following code chunk will stop parallel processing.

```{r}

# Stopping parallel 
stopCluster(cl)

```

# Evaluation on held-out test set 

The following code chunk will conduct evaluation on the held-out test set.

```{r evaluate, message=FALSE, warning=FALSE}

# Generating predictions and re-attach the true yield column
test_preds_xgb <- predict(final_xgb_fit, new_data = weather_test_xgb) %>%
  bind_cols(weather_test_xgb)

# Computing RMSE and R2 using yardstick (specify data = so `yield` is found)
rmse_res <- rmse(
  data = test_preds_xgb,
  truth = yield,    # the observed values
  estimate = .pred     # the predicted values
)

rsq_res <- rsq(
  data = test_preds_xgb,
  truth = yield,
  estimate = .pred
)

# Combining and printing
test_metrics <- bind_rows(rmse_res, rsq_res)
print(test_metrics)

```

# Predicted vs Observed plot

The following code chunk will Predicted vs Observed plot.

```{r}

# Predicted vs Observed plot 

metrics <- test_metrics %>%
  pivot_wider(names_from = .metric, values_from = .estimate) # wide format

ggplot(test_preds_xgb, aes(x = yield, y = .pred)) +
  geom_abline() +  
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(
    title = "XGBoost: Predicted vs Observed Yield",
    subtitle = paste0("R² = ", round(metrics$rsq, 3),
                      "   RMSE = ", round(metrics$rmse, 3)),
    x = "Observed Yield",
    y = "Predicted Yield"
  ) +
  theme_minimal()


```

The following code chunk will export "test_preds_xgb" as "test_preds_xgb.csv".

```{r}

write_csv(test_preds_xgb,
          "../data/test_preds_xgb.csv")

```

# Variable importance plot

The following code chunk will create Variable importance plot. 

```{r}

library(vip)
library(ggplot2)
library(forcats)
library(dplyr)

# Extracting the fitted xgboost model from your workflow
wf_fit <- pull_workflow_fit(final_xgb_fit)$fit

# Computing all variable‐importance scores
vi_df <- vi(wf_fit)

# Selecting the top 20 by absolute importance
vi_top20 <- vi_df %>%
  slice_max(order_by = Importance, n = 20)

# Plotting with ggplot2
ggplot(vi_top20, aes(
    x = Importance,
    y = fct_reorder(Variable, Importance)
  )) +
  geom_col() +
  labs(
    title = "XGBoost Variable Importance",
    x     = "Importance",
    y     = "Feature"
  ) +
  theme_minimal()


```

The following code chunk will export "final_xgb_fit" as "final_xgb_fit.rds".

```{r}

readr::write_rds(
  final_xgb_fit,
  file     = "../data/final_xgb_fit.rds",
  compress = "gz"
)
          
```

The following code chunk will export "vi_top20" as "vi_top20.csv".

```{r}

write_csv(vi_top20,
          "../data/vi_top20.csv")

```




