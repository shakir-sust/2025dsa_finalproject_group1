---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script contains ML Workflow with XGBoost

# Loading packages

The following code chunk will load necessary packages.

```{r}

# 1. Setup 

#install.packages("tidymodels")   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
#install.packages("finetune")     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
#install.packages("vip")          # For plotting variable importance from fitted models
#install.packages("xgboost")      # XGBoost implementation in R
#install.packages("ranger")       # Fast implementation of Random Forests
#install.packages("tidyverse")    # Data wrangling and visualization
#install.packages("doParallel")   # For parallel computing (useful during resampling/tuning)
#install.packages("caret")  
#install.packages("xgboost") #new pacakage
#install.packages("caret")
#install.packages("yardstick")

library(tidymodels)   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
library(finetune)     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
library(vip)          # For plotting variable importance from fitted models
library(xgboost)      # XGBoost implementation in R
library(ranger)       # Fast implementation of Random Forests
library(tidyverse)    # Data wrangling and visualization
library(doParallel)   # For parallel computing (useful during resampling/tuning)
library(caret)       # Other great library for Machine Learning 
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(stringr)
library(tidymodels)
library(xgboost)
library(vip)
library(finetune)
library(doParallel)
library(parallel)
library(beepr)
library(yardstick)  

```

# Loading the data set

The following code chunk will load the "weather_monthsum.csv" data set.

```{r weather}

# Data import 

weather_xgb <- read_csv("../data/weather_monthsum.csv") %>%
  rename(yield = adjusted_yield)

weather_xgb

```

# EDA plots on soil predictors

The following code chunk will create EDA plots on soil predictors.

```{r ridge_soil_selected_fixed, message=FALSE, warning=FALSE}

library(ggridges)   
library(viridis)    
library(purrr)       
library(ggplot2)  

var_labels <- c(
  soilpH = "soil pH",
  om_pct = "soil organic matter"
)

walk2(
  names(var_labels),    
  var_labels,           
  function(var, label) {
    # computing density to normalize height
    dens <- density(weather_xgb[[var]], na.rm = TRUE)
    scale_val <- 1 / max(dens$y, na.rm = TRUE)
    
    # building the ggplot object
    p <- ggplot(weather_xgb, aes(x = .data[[var]], y = label, fill = stat(x))) +
      geom_density_ridges_gradient(
        scale = scale_val,
        rel_min_height = 0.01
      ) +
      scale_fill_viridis_c(option = "C") +
      labs(
        title = paste("Distribution of", label),
        x = "Mean soil pH between 2014 - 2024",
        y = NULL
      ) +
      theme_ridges() +
      theme(
        legend.position = "none",
        axis.text.y = element_blank()
      )
    print(p)
  }
)

```

The following code chunk will export "weather_xgb.csv".

```{r}

write_csv(weather_xgb,
          "../data/weather_xgb.csv")

```

# Splitting off 2024 for final prediction

The following code chunk will split off 2024 for final prediction 

```{r}

# Split off 2024 for final prediction 

weather_to_predict <- weather_xgb %>% filter(is.na(yield)) # holding out 2024 rows (no yield)
weather_to_predict

weather_complete <- weather_xgb %>% filter(!is.na(yield)) # keeping 2014–2023 for training
weather_complete 

```

# Train/test split

The following code chunk will conduct train/test split for the data of the years 2014–2023.

```{r}

# Train/test split (2014–2023) 

set.seed(931735) # reproducible split

xgb_split <- initial_split(weather_complete,  # creating split object
                           prop = 0.7, # 70% train
                           strata = yield) # stratifying on yield
xgb_split

weather_train_xgb <- training(xgb_split) # extracting training set
weather_train_xgb

weather_test_xgb  <- testing(xgb_split) # extracting test set
weather_test_xgb 

```

# Comparing the distribution of "yield" in training vs. test sets

The following code chunk will compare the distribution of "yield" in training vs. test sets.

```{r distribution_with_legend, message=FALSE, warning=FALSE}

library(ggplot2)
library(dplyr)

# 1. Combine train and test with a new 'dataset' column
yield_df <- bind_rows(
  weather_train_xgb %>% mutate(dataset = "Training"),
  weather_test_xgb  %>% mutate(dataset = "Test")
)

# 2. Plot densities with a legend
ggplot(yield_df, aes(x = yield, color = dataset)) +
  geom_density(size = 1) +
  scale_color_manual(
    values = c(Training = "red", Test = "blue"),
    name = "Data split"
  ) +
  labs(
    title = "Distribution of Yield",
    x = "Yield (Mg/ha)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12),
    legend.position = "top"
  )
```

The following code chunk will export "yield_df.csv"

```{r}

write_csv(yield_df,
          "../data/yield_df.csv")

```

# Pre-processing recipe

The following code chunk will conduct pre-processing recipe.

```{r}

# Pre-processing recipe 

weather_recipe_xgb <-
  recipe(yield ~ ., data = weather_train_xgb) %>% # targeting ~ all predictors
  step_rm(year, site, hybrid, # dropping IDs
          matches("Jan|Feb|Mar|Nov|Dec")) %>% # dropping off‐season months
  step_impute_median(all_numeric_predictors()) # median imputation for NAs

weather_recipe_xgb

```

# Model specification

The following code chunk will conduct model specification.

```{r}

# Model specification 

xgb_spec <-
  boost_tree(
    trees = tune(), # tuning number of boosting rounds
    tree_depth = tune() # tuning max tree depth
  ) %>%
  set_engine("xgboost") %>%  # using XGBoost backend
  set_mode("regression") # regression mode

xgb_spec

```

# Bundle recipe & model into workflow

The following code chunk will bundle recipe & model into workflow.

```{r}

# Workflow 

xgb_wf <- workflow() %>%
  add_recipe(weather_recipe_xgb) %>% # adding preprocessing
  add_model(xgb_spec)  # adding XGBoost spec

xgb_wf

```

# Resampling strategy

The following code chunk will setup Resampling plan.  

```{r}

# Resampling setup 

set.seed(34549)

resamples_xgb <- vfold_cv(
  weather_train_xgb, 
  v = 5 # 5 folds
  )

resamples_xgb

```

# Hyperparameter grid search with Latin Hypercube Sampling

The following code chunk will conduct hyperparameter grid search with Latin Hypercube Sampling.

```{r}

# Hyperparameter grid (Latin hypercube) 

xgb_grid <- grid_latin_hypercube(
  trees(), # tuning number of trees
  tree_depth(), # tuning tree depth
  size = 10
)

xgb_grid

```
